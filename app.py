import streamlit as st
import pandas as pd
import pickle
import numpy as np
import re
import os
import ssl
import torch
import torchvision.transforms as T
import cv2
import requests
import base64
from PIL import Image, ImageEnhance, ImageDraw
from io import BytesIO
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing import image as k_image
from sklearn.metrics.pairwise import cosine_similarity
from streamlit_image_coordinates import streamlit_image_coordinates

# [0] í™˜ê²½ ì„¤ì •: SSL ìš°íšŒ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ ë³´ì•ˆ ëŒ€ì‘
ssl._create_default_https_context = ssl._create_unverified_context

# --- [1] ìœ í‹¸ë¦¬í‹° ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ ---
def get_direct_url(url):
    """êµ¬ê¸€ ë“œë¼ì´ë¸Œ URLì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
    if not url or str(url) == 'nan' or 'drive.google.com' not in url: return url
    if 'file/d/' in url: file_id = url.split('file/d/')[1].split('/')[0]
    elif 'id=' in url: file_id = url.split('id=')[1].split('&')[0]
    else: return url
    return f'https://drive.google.com/uc?export=download&id={file_id}'

def get_image_as_base64(url):
    """êµ¬ê¸€ ë³´ì•ˆ ìš°íšŒ: ì„œë²„ ì¸¡ì—ì„œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ base64ë¡œ ë³€í™˜"""
    try:
        r = requests.get(get_direct_url(url), timeout=10)
        img_str = base64.b64encode(r.content).decode()
        return f"data:image/png;base64,{img_str}"
    except: return None

def load_csv_smart(target_name):
    """4ê°€ì§€ ì¸ì½”ë”© ìë™ ì‹œë„ë¡œ UnicodeDecodeError ë°©ì–´"""
    files = os.listdir('.')
    for f in files:
        if f.lower() == target_name.lower():
            for enc in ['utf-8-sig', 'cp949', 'utf-8', 'euc-kr']:
                try: return pd.read_csv(f, encoding=enc)
                except: continue
    st.error(f"âŒ {target_name} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

def get_digits(text):
    return "".join(re.findall(r'\d+', str(text))) if text else ""

@st.cache_resource
def init_resources():
    # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë“œ (ResNet50 + DINOv2)
    model_res = ResNet50(weights='imagenet', include_top=False, pooling='avg')
    model_dino = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')
    model_dino.eval()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë° CSV ë¡œë“œ
    with open('material_features.pkl', 'rb') as f:
        feature_db = pickle.load(f)
        
    df_path = load_csv_smart('ì´ë¯¸ì§€ê²½ë¡œ.csv')
    df_info = load_csv_smart('í’ˆëª©ì •ë³´.csv')
    df_stock = load_csv_smart('í˜„ì¬ê³ .csv')
    
    # ì¬ê³  ì§‘ê³„ ë¡œì§ (ìœ  ëŒ€ë¦¬ë‹˜ ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    df_stock['ì¬ê³ ìˆ˜ëŸ‰'] = pd.to_numeric(df_stock['ì¬ê³ ìˆ˜ëŸ‰'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
    df_stock['í’ˆë²ˆ_KEY'] = df_stock['í’ˆë²ˆ'].astype(str).str.strip().str.upper()
    agg_stock = df_stock.groupby('í’ˆë²ˆ_KEY')['ì¬ê³ ìˆ˜ëŸ‰'].sum().to_dict()
    stock_date = str(int(df_stock['ì •ì‚°ì¼ì'].max())) if 'ì •ì‚°ì¼ì' in df_stock.columns else "í™•ì¸ë¶ˆê°€"
    
    return model_res, model_dino, feature_db, df_path, df_info, agg_stock, stock_date

# ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
res_model, dino_model, feature_db, df_path, df_info, agg_stock, stock_date = init_resources()

# DINOv2 ì „ìš© ì´ë¯¸ì§€ ë³€í™˜
dino_transform = T.Compose([
    T.Resize(224), T.CenterCrop(224), T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# --- [2] ì´ë¯¸ì§€ ê³ ë„í™” ì²˜ë¦¬ ì—”ì§„ (CLAHE & Warp) ---
def apply_clahe(img):
    """CLAHE(ë°ê¸° ê· ì¼í™”) ë„ì…: ì¡°ëª… ì°¨ì´ë¡œ ì¸í•œ ì¸ì‹ë¥  ì €í•˜ ë°©ì§€"""
    img_np = np.array(img)
    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    cl = clahe.apply(l)
    img_np = cv2.merge((cl, a, b))
    return Image.fromarray(cv2.cvtColor(img_np, cv2.COLOR_LAB2RGB))

def order_points(pts):
    rect = np.zeros((4, 2), dtype="float32")
    s = pts.sum(axis=1); rect[0] = pts[np.argmin(s)]; rect[2] = pts[np.argmax(s)]
    diff = np.diff(pts, axis=1); rect[1] = pts[np.argmin(diff)]; rect[3] = pts[np.argmax(diff)]
    return rect

def four_point_transform(image, pts):
    """LANCZOS4 ë³´ê°„ë²• ì ìš© ì›Œí•‘: ë‚˜ë­‡ê²° ë­‰ê°œì§ ë°©ì§€"""
    rect = order_points(pts)
    (tl, tr, br, bl) = rect
    w1 = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
    w2 = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
    h1 = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
    h2 = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
    mW, mH = max(int(w1), int(w2)), max(int(h1), int(h2))
    dst = np.array([[0, 0], [mW - 1, 0], [mW - 1, mH - 1], [0, mH - 1]], dtype="float32")
    M = cv2.getPerspectiveTransform(rect, dst)
    return cv2.warpPerspective(image, M, (mW, mH), flags=cv2.INTER_LANCZOS4)

def apply_smart_filters(img, category, lighting, brightness, sharpness):
    """UI ë³µêµ¬: ìì¬ ì¹´í…Œê³ ë¦¬(ì„ì¬ í¬í•¨) ë° í•„í„° ì ìš©"""
    if lighting == 'ë°±ì—´ë“± (ëˆ„ëŸ° ì¡°ëª…)':
        r, g, b = img.split(); b = b.point(lambda i: i * 1.2); img = Image.merge('RGB', (r, g, b))
    img = apply_clahe(img) # CLAHE ìë™ ì ìš©
    en_con = ImageEnhance.Contrast(img); en_shp = ImageEnhance.Sharpness(img); en_bri = ImageEnhance.Brightness(img)
    if category != 'ì¼ë°˜':
        img = en_shp.enhance(2.0); img = en_con.enhance(1.1)
    if brightness != 1.0: img = en_bri.enhance(brightness)
    if sharpness != 1.0: img = en_shp.enhance(sharpness)
    return img

# --- [3] ë©”ì¸ UI (ì „ë©´ ë³µêµ¬) ---
st.set_page_config(layout="wide", page_title="v3.4 í†µí•© ìì¬ ê²€ìƒ‰")
st.title("ğŸ­ í•˜ì´ë¸Œë¦¬ë“œ ìì¬ íŒ¨í„´ ê²€ìƒ‰ (v3.4)")
st.sidebar.info(f"ğŸ“… ì¬ê³  ê¸°ì¤€ì¼: {stock_date}")

if 'points' not in st.session_state: st.session_state['points'] = []
if 'search_done' not in st.session_state: st.session_state['search_done'] = False
if 'refresh_count' not in st.session_state: st.session_state['refresh_count'] = 0

# UI ë³µêµ¬: íŒŒì¼ ì—…ë¡œë“œ ë° ì œì–´ ë²„íŠ¼
uploaded = st.file_uploader("ğŸ“¸ ë¶„ì„í•  ìì¬ ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['jpg','png','jpeg'])

if uploaded:
    if 'current_img_name' not in st.session_state or st.session_state['current_img_name'] != uploaded.name:
        st.session_state['points'] = []; st.session_state['search_done'] = False
        st.session_state['current_img_name'] = uploaded.name
        st.session_state['proc_img'] = Image.open(uploaded).convert('RGB')
        st.rerun()

    working_img = st.session_state['proc_img']
    w, h = working_img.size
    
    st.markdown("### 1ï¸âƒ£ í™˜ê²½ ì„¤ì •")
    source_type = st.radio("ğŸ“‚ ì›ë³¸ ì¢…ë¥˜", ['ğŸ“¸ í˜„ì¥ ì‚¬ì§„', 'ğŸ’» ë””ì§€í„¸ íŒŒì¼'], horizontal=True) # ë³µêµ¬
    c_opt1, c_opt2 = st.columns(2)
    with c_opt1: mat_type = st.selectbox("ğŸ§± ìì¬ ì¢…ë¥˜", ['ì¼ë°˜', 'ë§ˆë£¨/ìš°ë“œ (Wood)', 'í•˜ì´ê·¸ë¡œì‹œ/ìœ ê´‘ (Glossy)', 'ë²½ì§€/íŒ¨ë¸Œë¦­ (Texture)', 'ì„ì¬/ì½˜í¬ë¦¬íŠ¸ (Stone)']) # ì„ì¬ ë³µêµ¬
    with c_opt2: s_mode = st.radio("ğŸ” ê²€ìƒ‰ ê¸°ì¤€", ["ğŸ¨ ì»¬ëŸ¬+íŒ¨í„´ ì¢…í•©(6:4)", "ğŸ¦“ íŒ¨í„´ ì¤‘ì‹¬ (í‘ë°±)"], horizontal=True) # ë³µêµ¬

    st.markdown("### 2ï¸âƒ£ ì˜ì—­ ì§€ì •")
    # UI ë³µêµ¬: ë³´ê¸° í¬ê¸° 10% ë‹¨ìœ„ ì œì–´
    scale = st.radio("ğŸ” ë³´ê¸° í¬ê¸° (ì¶•ì†Œ ê°€ëŠ¥):", [0.1, 0.3, 0.5, 0.7, 1.0], format_func=lambda x: f"{int(x*100)}%", index=3, horizontal=True)
    
    c_ref, c_del, c_auto = st.columns([1, 1, 2])
    with c_ref: 
        if st.button("ğŸ”„ ì´ë¯¸ì§€ ì•ˆë‚˜ì˜´"): # ë³µêµ¬
            st.session_state['refresh_count'] += 1; st.rerun()
    with c_del:
        if st.button("âŒ ì„ íƒ ì´ˆê¸°í™”"): st.session_state['points'] = []; st.rerun()
    with c_auto:
        if st.button("â¹ï¸ ì „ì²´ ì„ íƒ"):
            st.session_state['points'] = [(0, 0), (w, 0), (w, h), (0, h)]; st.rerun()

    # UI ë³µêµ¬: ìˆ«ì ë° ê°€ì´ë“œë¼ì¸ ì‹œê°í™”
    d_img = working_img.resize((int(w*scale), int(h*scale)), Image.Resampling.LANCZOS)
    draw = ImageDraw.Draw(d_img)
    for i, p in enumerate(st.session_state['points']):
        px, py = p[0]*scale, p[1]*scale
        draw.ellipse((px-8, py-8, px+8, py+8), fill='red', outline='white', width=2)
        draw.text((px + 10, py - 10), str(i + 1), fill='red') # ìˆ«ì ë³µêµ¬

    if len(st.session_state['points']) == 4:
        draw.polygon([tuple((p[0]*scale, p[1]*scale)) for p in order_points(np.array(st.session_state['points']))], outline='#00FF00', width=3) # ê°€ì´ë“œë¼ì¸ ë³µêµ¬

    value = streamlit_image_coordinates(d_img, key=f"click_{st.session_state['refresh_count']}")
    if value and len(st.session_state['points']) < 4:
        new_p = (value['x']/scale, value['y']/scale)
        if not st.session_state['points'] or st.session_state['points'][-1] != new_p:
            st.session_state['points'].append(new_p); st.rerun()

    if len(st.session_state['points']) == 4:
        st.markdown("#### ğŸ” ë¶„ì„ ì˜ì—­")
        warped = four_point_transform(np.array(working_img), np.array(st.session_state['points'], dtype="float32"))
        final_img = Image.fromarray(warped)
        final_img = apply_smart_filters(final_img, mat_type, 'ì¼ë°˜', 1.0, 1.5)
        if s_mode == "ğŸ¦“ íŒ¨í„´ ì¤‘ì‹¬ (í‘ë°±)": final_img = final_img.convert("L").convert("RGB")
        
        st.image(final_img, width=300, caption="AI ë¶„ì„ ëŒ€ìƒ")
        
        if st.button("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘", type="primary", use_container_width=True):
            with st.spinner('ResNet(ê²° 60%) + DINO(êµ¬ì¡° 40%) í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì¤‘...'): # ê°€ì¤‘ì¹˜ ë³µêµ¬
                # ì‚¬ìš©ì ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
                x_res = k_image.img_to_array(final_img.resize((224, 224)))
                q_res = res_model.predict(preprocess_input(np.expand_dims(x_res, axis=0)), verbose=0).flatten()
                
                d_in = dino_transform(final_img).unsqueeze(0)
                with torch.no_grad():
                    q_dino = dino_model(d_in).cpu().numpy().flatten()

                # í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ê³„ì‚° (0.6:0.4)
                all_results = []
                for fn, db_vec in feature_db.items():
                    db_res = db_vec[:2048]; db_dino = db_vec[2048:]
                    s_res = cosine_similarity([q_res], [db_res])[0][0]
                    s_dino = cosine_similarity([q_dino], [db_dino])[0][0]
                    total_sim = (s_res * 0.6) + (s_dino * 0.4)
                    
                    # ì •ë³´ ë§¤ì¹­
                    d_key = get_digits(fn)
                    url_row = df_path[df_path['ì¶”ì¶œëœ_í’ˆë²ˆ'].apply(get_digits) == d_key]
                    url = url_row['ì¹´ì¹´ì˜¤í†¡_ì „ì†¡ìš©_URL'].values[0] if not url_row.empty else None
                    
                    if url:
                        qty = agg_stock.get(d_key, 0)
                        all_results.append({'formal': fn.split('.')[0], 'score': total_sim, 'url': url, 'stock': qty})

                all_results.sort(key=lambda x: x['score'], reverse=True)
                st.session_state['search_results'] = all_results[:15]
                st.session_state['search_done'] = True; st.rerun()

# --- [4] ê²°ê³¼ ì¶œë ¥ (êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì•¡ë°• í•´ê²°) ---
if st.session_state.get('search_done'):
    st.markdown("---")
    res_data = st.session_state['search_results']
    cols = st.columns(5)
    for i, item in enumerate(res_data):
        with cols[i % 5]:
            # ì•¡ë°• í•´ê²°: ì„œë²„ì—ì„œ base64ë¡œ ì´ë¯¸ì§€ë¥¼ ì¸ì½”ë”©í•˜ì—¬ ì¶œë ¥
            b64_img = get_image_as_base64(item['url'])
            if b64_img: st.image(b64_img, use_container_width=True)
            else: st.warning("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¡œë“œ ë¶ˆê°€")
            st.markdown(f"**{item['formal']}**")
            st.caption(f"ìœ ì‚¬ë„: {item['score']:.1%}")
            st.info(f"ì¬ê³ : {item['stock']:,}m")
