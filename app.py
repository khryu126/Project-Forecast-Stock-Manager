import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing import image
from sklearn.metrics.pairwise import cosine_similarity
from PIL import Image

# ì•± ì œëª© ë° ì„¤ì •
st.set_page_config(page_title="ìì¬ íŒ¨í„´ ê²€ìƒ‰ê¸°", page_icon="ğŸ”")
st.title("ğŸ” ì‹¤ì‹œê°„ ìì¬ íŒ¨í„´ ê²€ìƒ‰")
st.write("í˜„ì¥ì—ì„œ ì°ì€ ì‚¬ì§„ì„ ì˜¬ë¦¬ë©´ ê°€ì¥ ìœ ì‚¬í•œ ìì¬ë¥¼ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.")

# 1. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ì—ëŸ¬ ë©”ì‹œì§€ ê°•í™” ë²„ì „)
@st.cache_resource
def load_resources():
    # íŒŒì¼ëª… ì •ì˜ (ëŒ€ë¦¬ë‹˜ì´ ì˜¬ë ¤ì£¼ì‹  ì´ë¦„ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨)
    pkl_file = 'ìì¬_ì§€ë¬¸_ì¥ë¶€_light.pkl'
    spec_file = 'ìŠ¤í™ì¸ì½”ë“œ_25.12.08.csv'
    link_file = 'ì œëª© ì—†ëŠ” ìŠ¤í”„ë ˆë“œì‹œíŠ¸ - ì‹œíŠ¸1.csv'

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    missing_files = []
    for f in [pkl_file, spec_file, link_file]:
        if not os.path.exists(f):
            missing_files.append(f)
    
    if missing_files:
        # ì–´ë–¤ íŒŒì¼ì´ ì—†ëŠ”ì§€ í™”ë©´ì— ì •í™•íˆ í‘œì‹œ
        st.error(f"âš ï¸ ì•„ë˜ íŒŒì¼ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_files)}")
        st.info(f"í˜„ì¬ í´ë”ì˜ íŒŒì¼ ëª©ë¡: {os.listdir()}")
        return None, None, None, None

    # ëª¨ë¸ ë¡œë“œ
    model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
    
    # í”¼í´ ë¡œë“œ
    with open(pkl_file, 'rb') as f:
        feature_dict = pickle.load(f)
    
    # ì—‘ì…€ ë¡œë“œ (ì¸ì½”ë”© ìë™ ì‹œë„)
    def read_csv_safe(path):
        for enc in ['cp949', 'utf-8-sig', 'euc-kr']:
            try:
                return pd.read_csv(path, encoding=enc)
            except:
                continue
        return None

    spec_df = read_csv_safe(spec_file)
    link_df = read_csv_safe(link_file)
    
    return model, feature_dict, spec_df, link_df

# ë¦¬ì†ŒìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤í–‰
model, feature_dict, spec_df, link_df = load_resources()

# ë°ì´í„°ê°€ ëª¨ë‘ ë¡œë“œë˜ì—ˆì„ ë•Œë§Œ ì‹¤í–‰
if model is not None:
    # 2. ì‚¬ì§„ ì—…ë¡œë“œ ì„¹ì…˜
    uploaded_file = st.file_uploader("ê°€êµ¬ ì‚¬ì§„ì„ ì´¬ì˜í•˜ê±°ë‚˜ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['jpg', 'jpeg', 'png'])

    if uploaded_file:
        img = Image.open(uploaded_file)
        st.image(img, caption='ì—…ë¡œë“œëœ ì‚¬ì§„', use_column_width=True)
        
        with st.spinner('ìœ ì‚¬í•œ ìì¬ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
            img_resized = img.resize((224, 224))
            x = image.img_to_array(img_resized)
            x = np.expand_dims(x, axis=0)
            x = preprocess_input(x)
            target_feat = model.predict(x).flatten()
            
            # ëŒ€ì¡° ì‘ì—…
            scores = [(f, cosine_similarity([target_feat], [feat])[0][0]) for f, feat in feature_dict.items()]
            top_results = sorted(scores, key=lambda x: x[1], reverse=True)[:3]
            
            st.subheader("âœ¨ ë¶„ì„ ê²°ê³¼ Top 3")
            for i, (fname, score) in enumerate(top_results):
                m = link_df[link_df['íŒŒì¼ëª…'] == fname]
                if not m.empty:
                    pumbun = m.iloc[0]['ì¶”ì¶œëœ_í’ˆë²ˆ']
                    url = m.iloc[0]['ì¹´ì¹´ì˜¤í†¡_ì „ì†¡ìš©_URL']
                    s = spec_df[spec_df['í’ˆë²ˆ'] == str(pumbun).strip()]
                    name = s.iloc[0]['í’ˆëª…'] if not s.empty else "ì •ë³´ì—†ìŒ"
                    
                    with st.expander(f"{i+1}ìˆœìœ„: {name} (ì¼ì¹˜ìœ¨ {score*100:.1f}%)"):
                        st.write(f"**í’ˆë²ˆ:** {pumbun}")
                        st.link_button("êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì‚¬ì§„ í™•ì¸", url)
else:
    st.warning("âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GitHubì— ì˜¬ë¦° íŒŒì¼ëª…ê³¼ ìœ„ ì½”ë“œì— ì íŒ ì´ë¦„ì´ ì™„ì „íˆ ë˜‘ê°™ì€ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
